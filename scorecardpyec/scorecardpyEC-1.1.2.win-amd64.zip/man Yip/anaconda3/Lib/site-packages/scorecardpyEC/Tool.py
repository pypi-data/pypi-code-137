# -*- coding: utf-8 -*-
"""
Created on Thu Jan  6 11:13:25 2022

@author: man Yip
"""
from itertools import combinations
import numpy as np
import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.metrics import roc_curve,roc_auc_score
from freqCut.cutter import freq_cut,cut_by_bins,freq_cut_array,value_counts_weight
  
    
def psi_bak(df_bin1,df_bin2):
    df_psi=df_bin1.merge(df_bin2,left_on=['variable','bin'],right_on=['variable','bin'])
    df_psi.loc[df_psi['count_distr_y'].isna(),'count_distr_y']=0.001
    df_psi['psi']=np.log(df_psi['count_distr_x']/df_psi['count_distr_y'])*(df_psi['count_distr_x']-df_psi['count_distr_y'])
    psi_value = df_psi.groupby('variable')['psi'].sum()
    return psi_value,df_psi

def psi(dat_arr,threshold_distr,min_distr,cutby=0,weight_arr=None,include_na='auto'):
    if weight_arr is None:
        cut_weight = None
    else:
        cut_weight = weight_arr[cutby]
    label_arr = freq_cut_array(dat_arr,threshold_distr,min_distr,cutby,cut_weight)
    dists = []
    for i,label in enumerate(label_arr):
        if weight_arr is None:
            tmp_weight = None
        else:
            tmp_weight = weight_arr[i]
        dist = value_counts_weight(label,tmp_weight)
        dists.append(dist)
        
    if include_na == 'auto':
        for dist in dists:
            if (np.nan in dist.keys()) and dist[np.nan] >= min_distr:
                include_na = True
                break
    if include_na == 'auto':
        include_na = False
    
    if include_na == False:
        for dist in dists:
            if np.nan in dist.keys():
                del dist[np.nan]
    max_psi = 0
    psi_dfs = []
    for i,dist1 in enumerate(dists):
        for j,dist2 in enumerate(dists):
            if i<j:
                psi = pd.concat([dist1,dist2],axis=1)
                psi.columns = ['dist%s'%i,'dist%s'%j]
                psi = psi.fillna(0.001)
                psi_value = np.log(psi['dist%s'%i]/psi['dist%s'%j])*(psi['dist%s'%i]-psi['dist%s'%j])
                psi_value = psi_value.sum()
                psi_dfs.append(psi)
                if psi_value > max_psi:
                    max_psi = psi_value
    return max_psi,psi_dfs

def del_high_corr(df_corr,thv=0.8):
    def _f1(df_corr):
        corrs = df_corr.iloc[0]
        samll_corr_cols = df_corr.columns[np.abs(corrs) < thv]
        tmp = df_corr.loc[df_corr.index.isin(samll_corr_cols),df_corr.columns.isin(samll_corr_cols)]
        return corrs.name,tmp
    save_cols = []
    while df_corr.shape[0]>1:
        col,df_corr = _f1(df_corr)
        save_cols.append(col)
    if df_corr.shape[0]>0:
        save_cols.append(df_corr.index[0])
    return save_cols

def vif(df):
    vif = pd.DataFrame()
    vif['features'] = df.columns
    if df.shape[1]>1:
        vif['VIF Factor'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]
    else:
        vif['VIF Factor']=0
    vif = vif.sort_values('VIF Factor',ascending=False)
    return vif


def KS(target,score,weight=None):
    if isinstance(target,np.ndarray):
        target=pd.Series(target)
    if weight is None:
        weight=pd.Series(np.ones_like(target),index=target.index)
    if isinstance(score,np.ndarray):
        score = pd.Series(score,index=target.index)
    weight.name='weight'
    target.name='y'
    score.name='score'
    df = pd.concat([score,target,weight],axis=1)
    df['y_mutli_w']=df['y']*df['weight']
    total = df.groupby(['score'])['weight'].sum()
    bad = df.groupby(['score'])['y_mutli_w'].sum()
    all_df = pd.DataFrame({'total':total, 'bad':bad})
    all_df['good'] = all_df['total'] - all_df['bad']
    all_df.reset_index(inplace=True)
    all_df = all_df.sort_values(by='score',ascending=False)
    all_df['badCumRate'] = all_df['bad'].cumsum() / all_df['bad'].sum()
    all_df['goodCumRate'] = all_df['good'].cumsum() / all_df['good'].sum()
    ks = all_df.apply(lambda x: x.goodCumRate - x.badCumRate, axis=1)
    return np.abs(ks).max()

def AUC(target,score,weight=None):
    if isinstance(target,np.ndarray):
        target=pd.Series(target)
    if weight is None:
        weight=pd.Series(np.ones_like(target),index=target.index)
    else:
        weight = weight.loc[target.index]
    if isinstance(score,np.ndarray):
        score = pd.Series(score,index=target.index)
    return roc_auc_score(target, score.loc[target.index],sample_weight=weight)


def LIFTn(target,score,n=10,weight=None):
    bins = freq_cut(score,0.01,0.008,weight)
    score_inter = cut_by_bins(score,bins)
    if isinstance(target,np.ndarray):
        target=pd.Series(target)
    if weight is None:
        weight=pd.Series(np.ones_like(target),index=target.index)
        
    weight.name='weight'
    target.name='y'
    score_inter.name='score'
    
    df = pd.concat([score_inter,target,weight],axis=1)
    df['y_mutli_w']=df['y']*df['weight']
    
    inter_count= df.groupby(['score'])['weight'].sum()
    inter_bad = df.groupby(['score'])['y_mutli_w'].sum()
    
    inter = pd.concat([inter_bad.cumsum(),inter_count.cumsum()],axis=1)
    inter.columns = ['cum_bad','cum_all']
    inter['cum_bad_rate'] = inter['cum_bad']/inter['cum_all']
    inter['cum_all_dist'] = inter['cum_all']/inter['cum_all'].iloc[-1]

    n = n/100
    inter1 = inter.query('cum_all_dist<=@n').iloc[-1]
    inter2 = inter.query('cum_all_dist>@n').iloc[0]
    
    if n-inter1.cum_all_dist <= inter2.cum_all_dist-n:
        return np.around(inter1.cum_bad_rate/inter.cum_bad_rate.iloc[-1],2)
    else:
        return np.around(inter2.cum_bad_rate/inter.cum_bad_rate.iloc[-1],2)
    
    
    
    
    
    
    
    